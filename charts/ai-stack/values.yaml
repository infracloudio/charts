# Default values for ai-stack.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Values for tgi: the text-generation-inference chart
# Reference: https://artifacthub.io/packages/helm/infracloud-charts/text-generation-inference?modal=values
tei:
  enabled: true

  config:
    modelID: "BAAI/bge-large-en-v1.5"

  env:
    - name: MAX_CLIENT_BATCH_SIZE
      value: "1024"
    - name: RUST_BACKTRACE
      value: "full"
    - name: HF_API_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-api-token
          key: HF_API_TOKEN

  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1

  strategy:
    type: Recreate

  service:
    type: LoadBalancer
    port: 80

  volumeMounts:
    - name: hf-cache
      mountPath: /data

  volumes:
    - name: hf-cache
      persistentVolumeClaim:
        claimName: hf-cache


# Values for tei: the text-embeddings-inference chart
# Reference: https://artifacthub.io/packages/helm/infracloud-charts/text-embeddings-inference?modal=values
tgi:
  enabled: true

  config:
    modelID: "Qwen/Qwen2-7B-Instruct"

  env:
    - name: MAX_INPUT_TOKENS
      value: "6144"
    - name: MAX_TOTAL_TOKENS
      value: "8192"
    - name: HF_API_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-api-token
          key: HF_API_TOKEN

  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1

  strategy:
    type: Recreate

  service:
    type: LoadBalancer
    port: 80

  volumeMounts:
    - name: hf-cache
      mountPath: /data

  volumes:
    - name: hf-cache
      persistentVolumeClaim:
        claimName: hf-cache
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: "1Gi"


# Values for vector: the chromadb chart
# Reference: https://artifacthub.io/packages/helm/chromadb-helm/chromadb?modal=values
vectordb:
  enabled: true

  service:
    type: LoadBalancer

huggingface:

  cache:
    enabled: true

    existingClaim: ""

    persistentVolumeClaim:
      ## Ref: http://kubernetes.io/docs/user-guide/persistent-volum
      accessModes:
        - ReadWriteMany

      name: "hf-cache"

      labels: {}

      annotations:
        nfs.io/storage-path: "hf/hub"

      storageClassName: "controller-nfs"

      size: "50Gi"


# Values for reranker: the text-generation-inference chart
# Reference: https://artifacthub.io/packages/helm/infracloud-charts/text-generation-inference?modal=values
reranker:
  enabled: true

  config:
    modelID: "BAAI/bge-reranker-large"

  env:
    - name: MAX_CLIENT_BATCH_SIZE
      value: "1024"
    - name: RUST_BACKTRACE
      value: "full"
    - name: HF_API_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-api-token
          key: HF_API_TOKEN

  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1

  strategy:
    type: Recreate

  service:
    type: LoadBalancer
    port: 80

  volumeMounts:
    - name: hf-cache
      mountPath: /data

  volumes:
    - name: hf-cache
      persistentVolumeClaim:
        claimName: hf-cache